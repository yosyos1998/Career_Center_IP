{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPCNJeDlqrih",
        "outputId": "744d8e16-ac41-4b79-a94e-54cc85540a12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-surprise->surprise) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-surprise->surprise) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-surprise->surprise) (1.10.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp39-cp39-linux_x86_64.whl size=3195756 sha256=3d672c4a59223e9add52919ac3a5432725998e9afd6a57f84ea6b6e6a49d2fc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/3a/46/9b17b3512bdf283c6cb84f59929cdd5199d4e754d596d22784\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.3 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Sh7C_EVBqSmX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import cross_validate, train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random sample of 10% of the job offers and job applications\n",
        "job_offers = pd.read_csv('FinalDataSetJobOffers.csv').sample(frac=0.1, random_state=42)\n",
        "job_seekers= pd.read_csv('CvDatasetFinal_3.csv').sample(frac=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "EeOyRCLirmwe"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the job offers dataframe\n",
        "print(\"Missing values in job_offers:\")\n",
        "print(job_offers.isna().sum())\n",
        "\n",
        "# Check for missing values in the job applications dataframe\n",
        "print(\"Missing values in job_applications:\")\n",
        "print(job_seekers.isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M0YvxkXsjfd",
        "outputId": "7f32fe68-ba65-4390-ea8c-37b5c67fef60"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in job_offers:\n",
            "Job post               0\n",
            "Company name           0\n",
            "Job description        0\n",
            "Required skills        0\n",
            "Location               0\n",
            "Company rating         0\n",
            "Company review         0\n",
            "Experience required    0\n",
            "dtype: int64\n",
            "Missing values in job_applications:\n",
            "Category             0\n",
            "Name                 0\n",
            "Email                0\n",
            "Phone                0\n",
            "Education            0\n",
            "Skills               0\n",
            "Experience           2\n",
            "Experience_Rating    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values in job_offers and job_applications dataframes\n",
        "job_offers.dropna(inplace=True)\n",
        "job_seekers.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "2xQYxG6ssD3O"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess job offers data\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "job_offers_matrix = tfidf.fit_transform(job_offers['Job description'])\n"
      ],
      "metadata": {
        "id": "8bOFFWY5spk6"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for generating content-based recommendations\n",
        "def content_based_recommendations(user_profile, job_offers_matrix, job_offers, n_recommendations=10):\n",
        "    user_profile_matrix = tfidf.transform([user_profile])\n",
        "    similarities = cosine_similarity(user_profile_matrix, job_offers_matrix)\n",
        "    similar_jobs_indices = similarities.argsort()[0][::-1]\n",
        "    recommended_jobs = job_offers.iloc[similar_jobs_indices][:n_recommendations]\n",
        "    return recommended_jobs"
      ],
      "metadata": {
        "id": "BUDFRhgXtwui"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def content_recommendations(profile, offers_df, top_n):\n",
        "    # Compute the cosine similarity between the job descriptions and the user profile\n",
        "    profile_vector = tfidf.transform([profile])\n",
        "    cosine_similarities = cosine_similarity(profile_vector, job_offers_matrix).flatten()\n",
        "    \n",
        "    # Sort the job offers by their cosine similarity to the user profile\n",
        "    offers_df['cosine_similarity'] = cosine_similarities\n",
        "    offers_df = offers_df.sort_values(by='cosine_similarity', ascending=False)\n",
        "    \n",
        "    # Return the top N job offers\n",
        "    top_offers = offers_df.head(top_n)\n",
        "    top_offers = top_offers[['job_id', 'description', 'cosine_similarity']]\n",
        "    return top_offers"
      ],
      "metadata": {
        "id": "GtdFivRX0BtN"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def hybrid_recommendations(user_profile, job_offers_matrix, job_offers, top_n=5):\n",
        "    # create a new CountVectorizer with the same vocabulary as the TfidfVectorizer for the user profile\n",
        "    cv = CountVectorizer(vocabulary=tfidf.vocabulary_)\n",
        "    \n",
        "    # transform the job descriptions to a count matrix\n",
        "    job_desc_count = cv.transform(job_offers['Job description'])\n",
        "    \n",
        "    # concatenate the user profile and job description count matrices\n",
        "    combined_matrix = scipy.sparse.vstack([tfidf_matrix, job_desc_count])\n",
        "    \n",
        "    # calculate the cosine similarity between the user profile and job offers\n",
        "    cosine_sim = cosine_similarity(combined_matrix)\n",
        "    \n",
        "    # get the indices of the top_n job offers\n",
        "    job_indices = cosine_sim[0].argsort()[-top_n-1:-1][::-1]\n",
        "    \n",
        "    # get the corresponding job IDs and descriptions\n",
        "    job_ids = job_offers.iloc[job_indices]['job_id'].values\n",
        "    job_descs = job_offers.iloc[job_indices]['description'].values\n",
        "    \n",
        "    print( list(zip(job_ids, job_descs)))\n"
      ],
      "metadata": {
        "id": "7zG8RuOktzX7"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(job_offers.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhN3SHNj0fhq",
        "outputId": "01a86cc7-6a2f-469f-e94d-3d6f74351bcf"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Job post', 'Company name', 'Job description', 'Required skills',\n",
            "       'Location', 'Company rating', 'Company review', 'Experience required'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "job_offers_matrix = vectorizer.fit_transform(job_offers['Job description'])\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me0RbGEQ0iK6",
        "outputId": "616d542b-167d-4bb8-99c5-74ae64f14ee0"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00' '000' '0009' ... 'zuora' 'zweig' 'âmanhattan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(job_offers_matrix.getnnz(axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gan7F2h04VU",
        "outputId": "8eb0badb-dca7-4321-ad40-9098d0848631"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9 52  1 ...  1  1  1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of job_offers_matrix:\", job_offers_matrix.shape)\n",
        "print(\"Shape of user_profile_matrix.T:\", user_profile_matrix.T.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOYr5BRc1psO",
        "outputId": "a1ae3137-cdd0-453c-f0b8-b28cdede4d3e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of job_offers_matrix: (3357, 8041)\n",
            "Shape of user_profile_matrix.T: (8041, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_profile = 'I am a software engineer with experience in Java, Python, and SQL'\n",
        "num_words = len(user_profile.split())\n",
        "print(num_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIfv2_MB2OX3",
        "outputId": "a3a4d279-56b0-4706-c103-7ad5025acb5f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of top job offers to recommend to the user\n",
        "n = 5\n",
        "\n",
        "# Create a new user profile string\n",
        "user_profile = \"python machine learning data analysis\"\n",
        "\n",
        "# Transform the user profile string into a matrix\n",
        "user_profile_matrix = vectorizer.transform([user_profile])\n",
        "\n",
        "# Calculate the cosine similarity between the user profile and job offers\n",
        "cosine_sim = cosine_similarity(user_profile_matrix, job_offers_matrix)\n",
        "\n",
        "# Get the indices of the top_n job offers\n",
        "top_n = cosine_sim.argsort()[0][-n:]\n",
        "\n",
        "# Get the corresponding job IDs and descriptions\n",
        "job_ids = job_offers.iloc[top_n]['Job post'].values\n",
        "job_descs = job_offers.iloc[top_n]['Job description'].values\n",
        "\n",
        "# Print the top_n job offers\n",
        "for job_id, job_desc in zip(job_ids, job_descs):\n",
        "    print(job_id)\n",
        "    print(job_desc)\n",
        "    print('---------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qfAK2ebzJVv",
        "outputId": "9fab6a0d-dbfb-47b3-e395-7f76a8739d8d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data scientist\n",
            "optim candid respons naval nuclear laboratori seek data scientist join team dedic collect transform data naval nuclear fleet analyt throughout laboratori present in-depth analysi clear concis manner data-ori approach problem solv decis make build predict model character variou system process refin verifi integr data use analysi data scientist display great collabor interperson skill written verbal lead improv project provid regular updat organ prefer skill includ good script program skill experi python r. experi data visual packag tableau sa rstudio understand machine-learn applic profici work databas\n",
            "---------------------\n",
            "consult analyt der\n",
            "experi data scienc machin learn python r. advanc program ...\n",
            "---------------------\n",
            "data scientist\n",
            "look data scientist analyz larg amount raw inform find pattern help improv compani reli build data product extract valuabl busi insight role highli analyt knack analysi math statist critic think problem-solv skill essenti interpret data also want see passion machine-learn research goal help compani analyz trend make better decis respons identifi valuabl data sourc autom collect process undertak preprocess structur unstructur data analyz larg amount inform discov trend pattern build predict model machine-learn algorithm combin model ensembl model present inform use data visual techniqu propos solut strategi busi challeng collabor engin product develop team requir proven experi data scientist data analyst experi data mine understand machine-learn oper research knowledg r sql python familiar scala java c++ asset experi use busi intellig tool e.g tableau data framework e.g hadoop analyt mind busi acumen strong math skill e.g statist algebra problem-solv aptitud excel commun present skill bsc/ba comput scienc engin relev field graduat degre data scienc quantit field prefer benefit health care plan medic dental vision paid time vacat sick public holiday train develop free food snack amaz compani grow quickli get work fun/interest group peopl real transpar rest compani complet ownership account role function area\n",
            "---------------------\n",
            "data scientist\n",
            "respons · identifi valuabl data sourc autom collect process · undertak preprocess structur unstructur data · analyz larg amount inform discov trend pattern · build predict model machine-learn algorithm · combin model ensembl model · present inform use data visual techniqu · propos solut strategi busi challeng · collabor engin product develop team requir · proven experi data scientist data analyst · experi data mine · understand machine-learn oper research · knowledg r sql python familiar scala java c++ asset · experi use busi intellig tool e.g tableau data framework e.g hadoop · analyt mind busi acumen · strong math skill e.g statist algebra · problem-solv aptitud · excel commun present skill · bsc/ba comput scienc engin relev field graduat degre data scienc quantit field prefer\n",
            "---------------------\n",
            "machin learn engin\n",
            "overview machin learning/deep learn engin experi scale mod ...\n",
            "---------------------\n"
          ]
        }
      ]
    }
  ]
}